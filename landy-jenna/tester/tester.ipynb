{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Testing Framework: Exercises\n",
    "\n",
    "## Looping Over `globals`\n",
    "\n",
    "**What happens if you run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__name__\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "dictionary changed size during iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5cdf28820db7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dictionary changed size during iteration"
     ]
    }
   ],
   "source": [
    "for name in globals():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happens if you run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__name__\n",
      "__doc__\n",
      "__package__\n",
      "__loader__\n",
      "__spec__\n",
      "__builtin__\n",
      "__builtins__\n",
      "_ih\n",
      "_oh\n",
      "_dh\n",
      "In\n",
      "Out\n",
      "get_ipython\n",
      "exit\n",
      "quit\n",
      "_\n",
      "__\n",
      "___\n",
      "_i\n",
      "_ii\n",
      "_iii\n",
      "_i1\n",
      "name\n",
      "_i2\n"
     ]
    }
   ],
   "source": [
    "name = None\n",
    "for name in globals():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why?**\n",
    "\n",
    "The first run results in an error because the `globals()` dictionary changes size during the course of the for loop. This is because when the variable `name` is set to the first key in `globals()` (i.e. the first iteration of the for loop), the variable `name` is *added* to the`globals()` dictionary!\n",
    "\n",
    "## Counting Results\n",
    "\n",
    "1.  **Modify the test framework so that it reports which tests passed, failed, or had errors and also reports a summary of how many tests produced each result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "md-indent": "    "
   },
   "outputs": [],
   "source": [
    "def run_tests(prefix):\n",
    "    all_names = [n for n in globals() if n.startswith(prefix)]\n",
    "    results = {\"pass\": 0, \"fail\": 0, \"error\": 0, \"skip\": 0, \"expected failure\": 0}\n",
    "    for name in all_names:\n",
    "        func = globals()[name]\n",
    "        kind = classify(func)\n",
    "        try:\n",
    "            if kind == \"skip\":\n",
    "                print(f\"skip: {name}\")\n",
    "                results[\"skip\"] += 1\n",
    "            else:\n",
    "                func()\n",
    "                print(f\"pass: {name}\")\n",
    "                results[\"pass\"] += 1\n",
    "        except AssertionError as e:\n",
    "            if kind == \"fail\":\n",
    "                print(f\"pass (expected failure): {name}\")\n",
    "                results[\"expected failure\"] += 1\n",
    "            else:\n",
    "                print(f\"fail: {name} {str(e)}\")\n",
    "                results[\"fail\"] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"error: {name} {str(e)}\")\n",
    "            results[\"error\"] += 1\n",
    "            \n",
    "    print(\"\\n\\nTesting Summary:\")\n",
    "    print(f\"pass {results['pass']}\")\n",
    "    print(f\"fail {results['fail']}\")\n",
    "    print(f\"error {results['error']}\")\n",
    "    print(f\"skip {results['skip']}\")\n",
    "    print(f\"expected failure {results['expected failure']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  **Write unit tests to check that your answer to part 1 works correctly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "md-indent": "    "
   },
   "outputs": [],
   "source": [
    "def sign(value):\n",
    "    if value < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def test_sign_negative():\n",
    "    assert sign(-3) == -1\n",
    "\n",
    "test_sign_negative.skip = True\n",
    "\n",
    "def test_sign_positive():\n",
    "    assert sign(19) == 1\n",
    "    \n",
    "def test_sign_zero():\n",
    "    assert sign(0) == 0\n",
    "    \n",
    "test_sign_zero.fail = True\n",
    "\n",
    "def test_sign_zero_again():\n",
    "    assert sign(0) == 0\n",
    "\n",
    "def test_sign_error():\n",
    "    assert sgn(1) == 1\n",
    "    \n",
    "def classify(func):\n",
    "    if hasattr(func, \"skip\") and func.skip:\n",
    "        return \"skip\"\n",
    "    if hasattr(func, \"fail\") and func.fail:\n",
    "        return \"fail\"\n",
    "    return \"run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "md-indent": "    "
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip: test_sign_negative\n",
      "pass: test_sign_positive\n",
      "pass (expected failure): test_sign_zero\n",
      "fail: test_sign_zero_again \n",
      "error: test_sign_error name 'sgn' is not defined\n",
      "\n",
      "\n",
      "Testing Summary:\n",
      "pass 1\n",
      "fail 1\n",
      "error 1\n",
      "skip 1\n",
      "expected failure 1\n"
     ]
    }
   ],
   "source": [
    "run_tests('test_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  **Think of another plausible way to interpret part 1 that *wouldn't* pass the tests you wrote for part 2.**\n",
    "\n",
    "    Here we're trying to break the framework\n",
    "\n",
    "## Failing on Purpose\n",
    "\n",
    "**Putting assertions into code to check that it is behaving correctly is called defensive programming. It's a good practice, but we should make sure those assertions are failing when they're supposed to, just as we should test our smoke detectors every once in a while.**\n",
    "\n",
    "**Modify the tester so that if a test function's docstring is `\"test:assert\"`, the test passes if it raises an `AssertionError` and fails if it does not. Tests whose docstring don't contain `\"test:assert\"` should behave as before.**\n",
    "\n",
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(prefix):\n",
    "    all_names = [n for n in globals() if n.startswith(prefix)]\n",
    "    results = {\"pass\": 0, \"fail\": 0, \"error\": 0, \"skip\": 0, \"expected failure\": 0}\n",
    "    for name in all_names:\n",
    "        func = globals()[name]\n",
    "        kind = classify(func)\n",
    "        try:\n",
    "            if kind == \"skip\":\n",
    "                print(f\"skip: {name}\")\n",
    "                results[\"skip\"] += 1\n",
    "            else:\n",
    "                func()\n",
    "                if func.__doc__ == \"test:assert\":\n",
    "                  print(f\"fail: {name}\")\n",
    "                  results[\"fail\"] += 1\n",
    "                else:\n",
    "                  print(f\"pass: {name}\")\n",
    "                  results[\"pass\"] += 1\n",
    "        except AssertionError as e:\n",
    "            if func.__doc__ == \"test:assert\":\n",
    "                print(f\"pass: {name}\")\n",
    "                results[\"pass\"] += 1\n",
    "            else:\n",
    "                print(f\"fail: {name} {str(e)}\")\n",
    "                results[\"fail\"] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"error: {name} {str(e)}\")\n",
    "            results[\"error\"] += 1\n",
    "            \n",
    "    print(\"\\n\\nTesting Summary:\")\n",
    "    print(f\"pass {results['pass']}\")\n",
    "    print(f\"fail {results['fail']}\")\n",
    "    print(f\"error {results['error']}\")\n",
    "    print(f\"skip {results['skip']}\")\n",
    "    print(f\"expected failure {results['expected failure']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass: test2_sign_zero\n",
      "fail: test2_sign_positive\n",
      "\n",
      "\n",
      "Testing Summary:\n",
      "pass 1\n",
      "fail 1\n",
      "error 0\n",
      "skip 0\n",
      "expected failure 0\n"
     ]
    }
   ],
   "source": [
    "# should raise AssertionError: expect to pass\n",
    "def test2_sign_zero():\n",
    "    \"\"\"test:assert\"\"\"\n",
    "    assert sign(0) == 0\n",
    "    \n",
    "# should not raise AssertionError: expect to fail\n",
    "def test2_sign_positive():\n",
    "    \"\"\"test:assert\"\"\"\n",
    "    assert sign(1) == 1\n",
    "    \n",
    "run_tests(\"test2_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Teardown\n",
    "\n",
    "**Testing frameworks often allow programmers to specify a `setup` function that is to be run before each test and a corresponding `teardown` function that is to be run after each test. (`setup` usually re-creates complicated test fixtures, while `teardown` functions are sometimes needed to clean up after tests, e.g., to close database connections or delete temporary files.)**\n",
    "\n",
    "**Modify the testing tool in this chapter so that if a file of tests contains a function called `setup` then the tool calls it exactly once before running each test in the file. Add a similar way to register a `teardown` function.**\n",
    "\n",
    "Greg said that dealing with loading files from the string names is hard. Instead, I'll continue searching global functions. This testing framework assumes that the `setup` and `teardown` functions would also be found in `globals()` if they did exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(prefix):\n",
    "    all_globals = globals()\n",
    "    all_names = [n for n in all_globals if n.startswith(prefix)]\n",
    "    if 'setup' in all_globals:\n",
    "      print('setting up...\\n')\n",
    "      setup()\n",
    "    else:\n",
    "      print('no setup\\n')\n",
    "    \n",
    "    results = {\"pass\": 0, \"fail\": 0, \"error\": 0, \"skip\": 0, \"expected failure\": 0}\n",
    "    for name in all_names:\n",
    "        func = globals()[name]\n",
    "        kind = classify(func)\n",
    "        try:\n",
    "            if kind == \"skip\":\n",
    "                print(f\"skip: {name}\")\n",
    "                results[\"skip\"] += 1\n",
    "            else:\n",
    "                func()\n",
    "                if func.__doc__ == \"test:assert\":\n",
    "                  print(f\"fail: {name}\")\n",
    "                  results[\"fail\"] += 1\n",
    "                else:\n",
    "                  print(f\"pass: {name}\")\n",
    "                  results[\"pass\"] += 1\n",
    "        except AssertionError as e:\n",
    "            if func.__doc__ == \"test:assert\":\n",
    "                print(f\"pass: {name}\")\n",
    "                results[\"pass\"] += 1\n",
    "            else:\n",
    "                print(f\"fail: {name} {str(e)}\")\n",
    "                results[\"fail\"] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"error: {name} {str(e)}\")\n",
    "            results[\"error\"] += 1\n",
    "    \n",
    "    if 'teardown' in all_globals:\n",
    "      print('\\ntearing down...')\n",
    "      teardown()\n",
    "    else:\n",
    "      print('\\nno teardown')\n",
    "            \n",
    "    print(\"\\n\\nTesting Summary:\")\n",
    "    print(f\"pass {results['pass']}\")\n",
    "    print(f\"fail {results['fail']}\")\n",
    "    print(f\"error {results['error']}\")\n",
    "    print(f\"skip {results['skip']}\")\n",
    "    print(f\"expected failure {results['expected failure']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no setup\n",
      "\n",
      "pass: test3_sign_zero\n",
      "pass: test3_sign_positive\n",
      "\n",
      "no teardown\n",
      "\n",
      "\n",
      "Testing Summary:\n",
      "pass 2\n",
      "fail 0\n",
      "error 0\n",
      "skip 0\n",
      "expected failure 0\n"
     ]
    }
   ],
   "source": [
    "# should raise AssertionError: expect to pass\n",
    "def test3_sign_zero():\n",
    "    \"\"\"test:assert\"\"\"\n",
    "    assert sign(0) == 0\n",
    "    \n",
    "def test3_sign_positive():\n",
    "    for pos in [1,2,3]:\n",
    "      assert sign(1) == 1\n",
    "\n",
    "run_tests(\"test3_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that the list `[1,2,3]` is some hard-to-load dataset or object that we want to use throughout multiple tests. We don't want to reload it for each test, so we can define it globally as part of the setup. But it is only useful in the context of testing, so we want to delete it globally in the teardown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up...\n",
      "\n",
      "pass: test3_sign_zero\n",
      "pass: test3_sign_positive\n",
      "\n",
      "tearing down...\n",
      "\n",
      "\n",
      "Testing Summary:\n",
      "pass 2\n",
      "fail 0\n",
      "error 0\n",
      "skip 0\n",
      "expected failure 0\n"
     ]
    }
   ],
   "source": [
    "def setup():\n",
    "    global test_pos_list \n",
    "    test_pos_list = [1,2,3]\n",
    "\n",
    "# should raise AssertionError: expect to pass\n",
    "def test3_sign_zero():\n",
    "    \"\"\"test:assert\"\"\"\n",
    "    assert sign(0) == 0\n",
    "    \n",
    "def test3_sign_positive():\n",
    "    for pos in test_pos_list:\n",
    "      assert sign(1) == 1\n",
    "      \n",
    "def teardown():\n",
    "    global test_pos_list\n",
    "    del test_pos_list\n",
    "    \n",
    "run_tests(\"test3_\")\n",
    "\n",
    "# check that teardown worked correctly\n",
    "assert 'test_pos_list' not in globals()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
